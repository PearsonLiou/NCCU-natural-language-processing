{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIxwe3J0zPux"
   },
   "source": [
    "## 統計碩二 108354021 柳瑞俞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qr_xVjnkx4yq",
    "outputId": "849b1aae-ac28-4cab-be72-eb9a3ba830ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GOyhyah31ihg"
   },
   "outputs": [],
   "source": [
    "X, Y = build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTwekcgPzU65"
   },
   "source": [
    "## Build model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DK6ZYzVpzA6L"
   },
   "outputs": [],
   "source": [
    "def build_dataset():\n",
    "  labels = []\n",
    "  instances = []\n",
    "  for label in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(label):\n",
    "      instances.append(movie_reviews.raw(fileid))\n",
    "      if label == 'pos':\n",
    "        labels.append(1)\n",
    "      else:\n",
    "        labels.append(0)\n",
    "  return instances, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uRSYMbGYzDi8"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "  def __init__(self):\n",
    "    self.k = 0.01  # Smoothing factor\n",
    "    self.feature_table = set()\n",
    "    self.y_counts = Counter()\n",
    "    self.x_y_counts = defaultdict(lambda: Counter())\n",
    "    self.num_instances = 0\n",
    "\n",
    "  def extract_features(self, instance) -> set:\n",
    "    return set(word_tokenize(instance))\n",
    "\n",
    "  def train(self, instances, labels):\n",
    "    for instance, label in zip(instances, labels):\n",
    "      self.y_counts[label] += 1\n",
    "      for word in self.extract_features(instance):\n",
    "        self.x_y_counts[word][label] += 1\n",
    "        self.feature_table.add(word)\n",
    "    self.num_instances = len(instances)\n",
    "    print(\"Number of features: %d\" % len(self.feature_table))\n",
    "\n",
    "  def smooth_prob(self, word, label):\n",
    "    return (self.x_y_counts[word][label] + self.k) / (self.y_counts[label] + 2 * self.k)\n",
    "\n",
    "  def predict(self, instance):\n",
    "    y_probs = Counter()\n",
    "    features = self.extract_features(instance)\n",
    "    for y in self.y_counts:\n",
    "      y_probs[y] = math.log(self.y_counts[y] / self.num_instances)\n",
    "      for word in self.feature_table:\n",
    "        prob = self.smooth_prob(word, y)\n",
    "        if word in features:\n",
    "          y_probs[y] += math.log(prob)          \n",
    "        else:\n",
    "          y_probs[y] += math.log(1.0 - prob)\n",
    "    return y_probs.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JbmDFGAlzG74"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def cross_validation(instances, labels, k, train_pred_func):\n",
    "  golden_labels = []\n",
    "  pred_labels = []\n",
    "  for fold in range(k):\n",
    "    training_instances = []\n",
    "    training_labels = []\n",
    "    test_instances = []\n",
    "    test_labels = []\n",
    "    for i in range(len(instances)):\n",
    "      if i % k == fold:\n",
    "        test_instances.append(instances[i])\n",
    "        test_labels.append(labels[i])\n",
    "      else:\n",
    "        training_instances.append(instances[i])\n",
    "        training_labels.append(labels[i])\n",
    "    pred_labels += train_pred_func(training_instances, training_labels, test_instances)\n",
    "    golden_labels += test_labels\n",
    "    #print(pred_labels)\n",
    "    #print(golden_labels)\n",
    "  print(\"Accuracy: %.4f\\nPrecision: %.4f\\nRecall: %.4f\\nF-score: %.4f\" % (\n",
    "      accuracy_score(golden_labels, pred_labels), \n",
    "      precision_score(golden_labels, pred_labels), \n",
    "      recall_score(golden_labels, pred_labels), \n",
    "      f1_score(golden_labels, pred_labels)))\n",
    "  print(confusion_matrix(golden_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1cEtSpFzk2E"
   },
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lA9NtTeYzKgp",
    "outputId": "b9872d7b-8360-4288-8644-358733155a6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[('My', 'PRP$'), ('black', 'JJ'), ('dog', 'NN'), ('ate', 'VB'), ('my', 'PRP$'), ('cake', 'NN'), ('yesterday', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "print(nltk.pos_tag(word_tokenize(\"My black dog ate my cake yesterday.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBgWkhtZzvbb",
    "outputId": "57dc60d0-d9a1-4e02-89b6-a9e0568dc671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "open\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('opened',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZe7gFyazvsH",
    "outputId": "84f09c44-2b3e-4fe1-8e3b-79d27c2ebb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'good', 'movie', 'see']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = \"This is a good movie to see\"\n",
    "words = word_tokenize(instance)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopword_list = stopwords.words('english')\n",
    "def remove_stopwords(tokens):\n",
    " tokens_clean = []\n",
    " for tok in tokens:\n",
    "  if tok not in stopword_list:\n",
    "   tokens_clean.append(tok)\n",
    " return tokens_clean\n",
    "cc = remove_stopwords(words)\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JnZ71TbzvxP",
    "outputId": "0387e3d4-999b-4ef7-870d-52135f1f35e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation) \n",
    "def remove_punctuation_marks(tokens):\n",
    " clean_tokens = []\n",
    " for tok in tokens:\n",
    "  if tok not in string.punctuation:\n",
    "    clean_tokens.append(tok)\n",
    " return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5sCeYLZGzv0i",
    "outputId": "5d611fcb-c45f-4cc3-cc46-979b744c67a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "print(snowball_stemmer.stem('opened'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6-P_5py0HVg"
   },
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5H-XS9B1zvum",
    "outputId": "bfc9bdfa-ca9d-4f8d-f24e-4491d7dffdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_so', 'love_review', 'movie_first', 'trigram_first_movie_review', 'first_love', 'bigram_so_much', 'review_love', 'bigram_movie_so', 'trigram_my_first_movie', 'love', 'so', 'bigram_first_movie', 'bigram_This_is', 'trigram_movie_so_much', 'bigram_my_first', 'bigram_is_my', 'bigram_review_I', 'first_movie', 'review_movie', 'trigram_movie_review_I', 'is_first', 'love_so', 'much_love', 'love_movie', 'much', 'movie_love', 'so_love', 'is_movie', 'trigram_this_movie_so', 'bigram_movie_review', 'love_is', 'trigram_love_this_movie', 'much_so', 'so_movie', 'first_is', 'love_much', 'trigram_This_is_my', 'trigram_I_love_this', 'so_first', 'review', 'so_much', 'movie', 'movie_review', 'first', 'bigram_love_this', 'first_so', 'bigram_this_movie', 'trigram_is_my_first', 'is_love', 'bigram_I_love', 'love_first', 'movie_much', 'trigram_review_I_love', 'much_movie', 'is', 'movie_is'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class MyNBC3(NaiveBayesClassifier):\n",
    "  '''Define your own feature extraction function'''\n",
    "  def extract_features(self, instance) -> set:\n",
    "    words = word_tokenize(instance)\n",
    "    #words = remove_stopwords(words) \n",
    "    words = remove_punctuation_marks(words)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    features = set()\n",
    "    bigram_list = set()\n",
    "    speech_dict = ['J', 'V', 'N', 'R']\n",
    "    bigram_speech_list = list(map(list,combinations(speech_dict,2)))\n",
    "    q,p = [],[]\n",
    "\n",
    "    for bigram in bigram_speech_list:\n",
    "      locals()[f\"{bigram[0]}_{bigram[1]}_word\"] = []\n",
    "\n",
    "    for w, t in tags:\n",
    "      # Focus on only adjectives (J), adverbs (R), verbs (V), and nouns (N).\n",
    "      if t[0] in {'J', 'R', 'V', 'N'}:    \n",
    "        features.add(w)\n",
    "      for bigram_speech in bigram_speech_list:\n",
    "        if t[0] in bigram_speech:\n",
    "          locals()[f\"{bigram_speech[0]}_{bigram_speech[1]}_word\"].append(w)\n",
    "\n",
    "      q.append(t)\n",
    "      p.append(w)\n",
    "      for i in range(len(p) - 1):\n",
    "        if (q[i][0],q[i+1][0]) == ('J', 'N'):\n",
    "          features.add(\"%s_%s\" % (p[i],p[i+1]))\n",
    "        elif (q[i][0],q[i+1][0]) == ('R', 'V'):\n",
    "          features.add(\"%s_%s\" % (p[i],p[i+1]))\n",
    "        elif (q[i][0],q[i+1][0]) == ('V', 'R'):\n",
    "          features.add(\"%s_%s\" % (p[i],p[i+1]))\n",
    "        elif (q[i][0],q[i+1][0]) == ('V', 'N'):\n",
    "          features.add(\"%s_%s\" % (p[i],p[i+1]))\n",
    "        elif (q[i][0],q[i+1][0]) == ('V', 'I'):\n",
    "          features.add(\"%s_%s\" % (p[i],p[i+1]))\n",
    "        elif (q[i][0],q[i+1][0]) == ('J', 'R'):\n",
    "          features.add(\"%s_%s\" % (p[i],p[i+1]))\n",
    "\n",
    "\n",
    "    for bigram_speech in bigram_speech_list:\n",
    "      bigram_list = locals()[f\"{bigram_speech[0]}_{bigram_speech[1]}_word\"]\n",
    "\n",
    "      for k in range(len(bigram_list)-1):\n",
    "        features.add(\"%s_%s\" % (bigram_list[k], bigram_list[k+1]))\n",
    "        features.add(\"%s_%s\" % (bigram_list[k+1], bigram_list[k]))\n",
    "\n",
    "    for i in range(len(words) - 1):\n",
    "      features.add(\"bigram_%s_%s\" % (words[i], words[i+1]))\n",
    "\n",
    "    for i in range(len(words) - 2):\n",
    "      features.add(\"trigram_%s_%s_%s\" % (words[i], words[i+1], words[i+2]))\n",
    "    #for i in range(len(words) - 3):\n",
    "     # features.add(\"fourgram_%s_%s_%s_%s\" % (words[i], words[i+1], words[i+2], words[i+3]))\n",
    "    #for i in range(len(words) - 4):\n",
    "     # features.add(\"fifgram_%s_%s_%s_%s_%s\" % (words[i], words[i+1], words[i+2], words[i+3], words[i+4]))\n",
    "\n",
    "    return features\n",
    "\n",
    "  def train(self, instances, labels):\n",
    "    for instance, label in zip(instances, labels):\n",
    "      self.y_counts[label] += 1\n",
    "      for f in self.extract_features(instance):\n",
    "        self.x_y_counts[f][label] += 1\n",
    "        self.feature_table.add(f)\n",
    "    self.num_instances = len(instances)\n",
    "\n",
    "    # Reduce less frequent features with a threshold of 5 occurrences.\n",
    "\n",
    "    for f in self.x_y_counts:\n",
    "      if sum(self.x_y_counts[f].values()) < 3:\n",
    "        self.feature_table.remove(f)\n",
    "    print(\"Number of features: %d\" % len(self.feature_table))\n",
    "\n",
    "clf = MyNBC3()\n",
    "print(clf.extract_features(\"This is my first movie review. I love this movie so much!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vHOWUTlzver",
    "outputId": "90872af6-d5ff-4329-a692-223f49481329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 220695\n",
      "Number of features: 215789\n",
      "Number of features: 219053\n",
      "Number of features: 218186\n",
      "Number of features: 220793\n",
      "Accuracy: 0.8575\n",
      "Precision: 0.8607\n",
      "Recall: 0.8530\n",
      "F-score: 0.8569\n",
      "[[862 138]\n",
      " [147 853]]\n"
     ]
    }
   ],
   "source": [
    "def train_then_predict_mynbc(training_instances, training_labels, test_instances):\n",
    "  clf3 = MyNBC3()\n",
    "  clf3.train(training_instances, training_labels)\n",
    "  pred = []\n",
    "  for test_instance in test_instances:\n",
    "    pred.append(clf3.predict(test_instance))\n",
    "  return pred\n",
    "\n",
    "cross_validation(X, Y, 5, train_then_predict_mynbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLDtfJ5TzvaJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3-FYpdyzOp3"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "classification_HW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
