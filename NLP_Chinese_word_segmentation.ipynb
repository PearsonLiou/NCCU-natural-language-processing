{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data and Prepare training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942571"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "remote_url = \"https://raw.githubusercontent.com/hhhuang/nlp2019fall/master/word_segmentation/\"\n",
    "\n",
    "r = requests.get(remote_url + \"data/as_training.utf8\", allow_redirects=True)\n",
    "open('as_training.utf8', 'wb').write(r.content)\n",
    "\n",
    "r = requests.get(remote_url + \"data/as_testing_gold.utf8\", allow_redirects=True)\n",
    "open('as_testing_gold.utf8', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training data: 708953\n",
      "Number of sentences in the test data: 14432\n"
     ]
    }
   ],
   "source": [
    "raw_train = []\n",
    "raw_test = []\n",
    "with open(\"as_training.utf8\", encoding=\"utf8\") as fin:\n",
    "    for line in fin:\n",
    "        raw_train.append(line.strip().split(\"　\"))   # It is a full white space\n",
    "\n",
    "with open(\"as_testing_gold.utf8\", encoding=\"utf8\") as fin:\n",
    "    for line in fin:\n",
    "        raw_test.append(line.strip().split(\"　\"))   # It is a full white space\n",
    "\n",
    "print(\"Number of sentences in the training data: %d\" % len(raw_train))\n",
    "print(\"Number of sentences in the test data: %d\" % len(raw_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\CHRIST~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.243 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區長', '青學苑', '多', '開設', '有書法', '、', '插花', '、', '土風', '舞班', '，']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "print(list(jieba.cut(\"\".join(raw_test[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Your Own Chinese Word Segmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training instances for sequence labeling by converting a list of words to a sequence of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許', '多', '社', '區', '長', '青', '學', '苑', '多', '開', '設', '有', '書', '法', '、', '插', '花', '、', '土', '風', '舞', '班', '，']\n",
      "['L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'M', 'M', 'R', 'S']\n"
     ]
    }
   ],
   "source": [
    "def words_to_tags(words):\n",
    "    tags = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            tags.append('S')\n",
    "        else:\n",
    "            for i in range(len(word)):\n",
    "                if i == 0:\n",
    "                    tags.append('L')\n",
    "                elif i == len(word) - 1:\n",
    "                    tags.append('R')\n",
    "                else:\n",
    "                    tags.append('M')\n",
    "    return tags\n",
    "    \n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for sent in raw_train:\n",
    "    train_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence as a sequence of characters\n",
    "    train_Y.append(words_to_tags(sent))\n",
    "    \n",
    "for sent in raw_test:\n",
    "    test_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence\n",
    "    test_Y.append(words_to_tags(sent))\n",
    "    \n",
    "print(test_X[0])\n",
    "print(test_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CRF model for word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from sklearn-crfsuite) (1.12.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in d:\\anaconda\\lib\\site-packages (from sklearn-crfsuite) (4.47.0)\n",
      "Collecting python-crfsuite>=0.8.3\n",
      "  Downloading python_crfsuite-0.9.7-cp38-cp38-win_amd64.whl (156 kB)\n",
      "Installing collected packages: tabulate, python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6 tabulate-0.8.7\n"
     ]
    }
   ],
   "source": [
    "#!pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████████████████████████████████| 708953/708953 [18:28<00:00, 639.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 10.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 668078\n",
      "Seconds required: 113.456\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 1.000000\n",
      "num_memories: 6\n",
      "max_iterations: 150\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=15.29 loss=9521412.85 active=666051 feature_norm=1.00\n",
      "Iter 2   time=14.82 loss=5941119.32 active=668078 feature_norm=5.09\n",
      "Iter 3   time=15.37 loss=5608706.98 active=668078 feature_norm=5.62\n",
      "Iter 4   time=7.40  loss=5288120.25 active=668078 feature_norm=6.42\n",
      "Iter 5   time=7.17  loss=3858885.29 active=668078 feature_norm=12.43\n",
      "Iter 6   time=7.67  loss=3567340.64 active=668078 feature_norm=14.76\n",
      "Iter 7   time=7.78  loss=3329652.89 active=668078 feature_norm=14.68\n",
      "Iter 8   time=7.46  loss=3151551.35 active=668078 feature_norm=16.51\n",
      "Iter 9   time=7.52  loss=2935182.17 active=668078 feature_norm=19.32\n",
      "Iter 10  time=7.74  loss=2528400.16 active=668078 feature_norm=26.75\n",
      "Iter 11  time=14.86 loss=2421926.38 active=668078 feature_norm=29.69\n",
      "Iter 12  time=7.58  loss=2362909.11 active=668078 feature_norm=30.21\n",
      "Iter 13  time=7.82  loss=2291285.62 active=668078 feature_norm=32.62\n",
      "Iter 14  time=7.27  loss=2168026.08 active=668078 feature_norm=39.25\n",
      "Iter 15  time=7.57  loss=2159217.67 active=668078 feature_norm=48.12\n",
      "Iter 16  time=7.65  loss=2063685.38 active=668078 feature_norm=48.53\n",
      "Iter 17  time=7.41  loss=2045780.17 active=668078 feature_norm=48.94\n",
      "Iter 18  time=7.56  loss=2007225.98 active=668078 feature_norm=50.31\n",
      "Iter 19  time=7.20  loss=1941309.14 active=668078 feature_norm=52.87\n",
      "Iter 20  time=8.44  loss=1857957.99 active=668078 feature_norm=55.82\n",
      "Iter 21  time=15.82 loss=1829117.59 active=668078 feature_norm=57.42\n",
      "Iter 22  time=8.21  loss=1755820.22 active=668078 feature_norm=60.00\n",
      "Iter 23  time=8.29  loss=1709332.72 active=668078 feature_norm=61.48\n",
      "Iter 24  time=7.89  loss=1667630.59 active=668078 feature_norm=62.40\n",
      "Iter 25  time=7.82  loss=1612756.58 active=668078 feature_norm=64.12\n",
      "Iter 26  time=15.83 loss=1586197.51 active=668078 feature_norm=63.37\n",
      "Iter 27  time=7.92  loss=1548579.45 active=668078 feature_norm=63.46\n",
      "Iter 28  time=8.43  loss=1524319.46 active=668078 feature_norm=63.42\n",
      "Iter 29  time=8.45  loss=1502419.35 active=668078 feature_norm=63.48\n",
      "Iter 30  time=9.11  loss=1462913.56 active=668078 feature_norm=64.46\n",
      "Iter 31  time=8.31  loss=1420564.35 active=668078 feature_norm=67.86\n",
      "Iter 32  time=9.87  loss=1374961.59 active=668078 feature_norm=69.75\n",
      "Iter 33  time=9.77  loss=1352754.29 active=668078 feature_norm=70.61\n",
      "Iter 34  time=9.46  loss=1337660.71 active=668078 feature_norm=71.85\n",
      "Iter 35  time=7.87  loss=1311376.48 active=668078 feature_norm=73.28\n",
      "Iter 36  time=8.35  loss=1270514.21 active=668078 feature_norm=77.70\n",
      "Iter 37  time=8.93  loss=1249332.37 active=668078 feature_norm=78.12\n",
      "Iter 38  time=8.03  loss=1235549.86 active=668078 feature_norm=77.91\n",
      "Iter 39  time=8.39  loss=1211506.25 active=668078 feature_norm=78.19\n",
      "Iter 40  time=10.15 loss=1205243.15 active=668078 feature_norm=79.46\n",
      "Iter 41  time=8.52  loss=1187808.43 active=668078 feature_norm=79.78\n",
      "Iter 42  time=8.22  loss=1179418.96 active=668078 feature_norm=80.37\n",
      "Iter 43  time=8.22  loss=1171646.74 active=668078 feature_norm=81.32\n",
      "Iter 44  time=9.76  loss=1159421.45 active=668078 feature_norm=82.58\n",
      "Iter 45  time=8.76  loss=1144302.18 active=668078 feature_norm=86.13\n",
      "Iter 46  time=8.80  loss=1120073.85 active=668078 feature_norm=88.51\n",
      "Iter 47  time=9.30  loss=1105430.68 active=668078 feature_norm=88.26\n",
      "Iter 48  time=9.04  loss=1087803.38 active=668078 feature_norm=88.67\n",
      "Iter 49  time=9.17  loss=1083238.67 active=668078 feature_norm=90.19\n",
      "Iter 50  time=9.04  loss=1068433.27 active=668078 feature_norm=90.83\n",
      "Iter 51  time=9.24  loss=1054393.91 active=668078 feature_norm=92.04\n",
      "Iter 52  time=9.52  loss=1037397.62 active=668078 feature_norm=94.43\n",
      "Iter 53  time=8.58  loss=1015100.05 active=668078 feature_norm=97.77\n",
      "Iter 54  time=16.77 loss=1009337.69 active=668078 feature_norm=98.64\n",
      "Iter 55  time=8.65  loss=999711.27 active=668078 feature_norm=100.25\n",
      "Iter 56  time=9.86  loss=988663.03 active=668078 feature_norm=101.33\n",
      "Iter 57  time=10.12 loss=975289.61 active=668078 feature_norm=102.66\n",
      "Iter 58  time=9.28  loss=966524.19 active=668078 feature_norm=102.47\n",
      "Iter 59  time=8.47  loss=959545.88 active=668078 feature_norm=102.25\n",
      "Iter 60  time=8.62  loss=947551.79 active=668078 feature_norm=101.94\n",
      "Iter 61  time=9.80  loss=939159.66 active=668078 feature_norm=102.76\n",
      "Iter 62  time=10.68 loss=934022.41 active=668078 feature_norm=104.39\n",
      "Iter 63  time=8.86  loss=926993.64 active=668078 feature_norm=104.94\n",
      "Iter 64  time=9.34  loss=924764.49 active=668078 feature_norm=105.16\n",
      "Iter 65  time=9.23  loss=916644.95 active=668078 feature_norm=106.44\n",
      "Iter 66  time=19.01 loss=911316.02 active=668078 feature_norm=107.59\n",
      "Iter 67  time=8.36  loss=898740.84 active=668078 feature_norm=110.03\n",
      "Iter 68  time=9.36  loss=885825.17 active=668078 feature_norm=112.90\n",
      "Iter 69  time=9.84  loss=881431.41 active=668078 feature_norm=116.30\n",
      "Iter 70  time=10.71 loss=871417.44 active=668078 feature_norm=116.28\n",
      "Iter 71  time=10.45 loss=866465.20 active=668078 feature_norm=116.18\n",
      "Iter 72  time=10.02 loss=857991.35 active=668078 feature_norm=116.99\n",
      "Iter 73  time=10.29 loss=848621.53 active=668078 feature_norm=119.59\n",
      "Iter 74  time=10.43 loss=840909.11 active=668078 feature_norm=121.34\n",
      "Iter 75  time=10.25 loss=835431.58 active=668078 feature_norm=122.62\n",
      "Iter 76  time=9.81  loss=830992.07 active=668078 feature_norm=123.78\n",
      "Iter 77  time=10.35 loss=823334.86 active=668078 feature_norm=125.67\n",
      "Iter 78  time=10.11 loss=814442.55 active=668078 feature_norm=127.66\n",
      "Iter 79  time=10.08 loss=808112.43 active=668078 feature_norm=129.65\n",
      "Iter 80  time=9.70  loss=803148.03 active=668078 feature_norm=129.87\n",
      "Iter 81  time=9.65  loss=800631.06 active=668078 feature_norm=129.65\n",
      "Iter 82  time=9.88  loss=790067.05 active=668078 feature_norm=131.02\n",
      "Iter 83  time=8.97  loss=778617.50 active=668078 feature_norm=133.82\n",
      "Iter 84  time=15.32 loss=775409.84 active=668078 feature_norm=135.48\n",
      "Iter 85  time=8.42  loss=767694.59 active=668078 feature_norm=138.51\n",
      "Iter 86  time=8.81  loss=762180.79 active=668078 feature_norm=141.01\n",
      "Iter 87  time=8.91  loss=760108.60 active=668078 feature_norm=144.67\n",
      "Iter 88  time=8.96  loss=752989.96 active=668078 feature_norm=145.21\n",
      "Iter 89  time=8.52  loss=749722.28 active=668078 feature_norm=144.95\n",
      "Iter 90  time=8.57  loss=746266.48 active=668078 feature_norm=144.88\n",
      "Iter 91  time=9.00  loss=741095.94 active=668078 feature_norm=146.63\n",
      "Iter 92  time=17.73 loss=735034.07 active=668078 feature_norm=147.66\n",
      "Iter 93  time=8.60  loss=726006.32 active=668078 feature_norm=152.63\n",
      "Iter 94  time=8.60  loss=721237.88 active=668078 feature_norm=154.81\n",
      "Iter 95  time=8.72  loss=714331.50 active=668078 feature_norm=157.76\n",
      "Iter 96  time=19.16 loss=712192.02 active=668078 feature_norm=157.37\n",
      "Iter 97  time=9.31  loss=708454.51 active=668078 feature_norm=156.91\n",
      "Iter 98  time=8.83  loss=703931.23 active=668078 feature_norm=155.79\n",
      "Iter 99  time=8.94  loss=701306.64 active=668078 feature_norm=155.86\n",
      "Iter 100 time=9.37  loss=698958.81 active=668078 feature_norm=157.77\n",
      "Iter 101 time=8.92  loss=694096.56 active=668078 feature_norm=158.76\n",
      "Iter 102 time=9.00  loss=689616.54 active=668078 feature_norm=160.94\n",
      "Iter 103 time=9.18  loss=683595.57 active=668078 feature_norm=164.88\n",
      "Iter 104 time=9.13  loss=679048.34 active=668078 feature_norm=170.71\n",
      "Iter 105 time=8.86  loss=675146.05 active=668078 feature_norm=173.93\n",
      "Iter 106 time=8.75  loss=673482.56 active=668078 feature_norm=173.47\n",
      "Iter 107 time=8.94  loss=670148.77 active=668078 feature_norm=173.44\n",
      "Iter 108 time=9.42  loss=666969.71 active=668078 feature_norm=174.49\n",
      "Iter 109 time=8.11  loss=663617.63 active=668078 feature_norm=178.29\n",
      "Iter 110 time=7.40  loss=657306.37 active=668078 feature_norm=180.54\n",
      "Iter 111 time=7.53  loss=654638.03 active=668078 feature_norm=182.72\n",
      "Iter 112 time=8.04  loss=652870.90 active=668078 feature_norm=184.48\n",
      "Iter 113 time=7.24  loss=648935.96 active=668078 feature_norm=187.88\n",
      "Iter 114 time=8.25  loss=647414.87 active=668078 feature_norm=194.56\n",
      "Iter 115 time=8.70  loss=642907.27 active=668078 feature_norm=193.16\n",
      "Iter 116 time=7.41  loss=641720.85 active=668078 feature_norm=192.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 117 time=7.42  loss=638789.95 active=668078 feature_norm=192.36\n",
      "Iter 118 time=7.62  loss=636433.85 active=668078 feature_norm=193.05\n",
      "Iter 119 time=7.57  loss=631863.81 active=668078 feature_norm=194.69\n",
      "Iter 120 time=7.59  loss=629039.57 active=668078 feature_norm=196.84\n",
      "Iter 121 time=7.25  loss=626447.85 active=668078 feature_norm=198.89\n",
      "Iter 122 time=7.39  loss=622299.94 active=668078 feature_norm=201.86\n",
      "Iter 123 time=7.08  loss=619241.79 active=668078 feature_norm=204.75\n",
      "Iter 124 time=7.91  loss=617767.01 active=668078 feature_norm=203.71\n",
      "Iter 125 time=8.70  loss=615849.71 active=668078 feature_norm=202.56\n",
      "Iter 126 time=7.22  loss=613186.50 active=668078 feature_norm=202.07\n",
      "Iter 127 time=15.05 loss=610653.88 active=668078 feature_norm=203.45\n",
      "Iter 128 time=7.09  loss=605939.12 active=668078 feature_norm=204.41\n",
      "Iter 129 time=7.15  loss=603498.64 active=668078 feature_norm=206.19\n",
      "Iter 130 time=7.17  loss=601765.70 active=668078 feature_norm=208.09\n",
      "Iter 131 time=7.75  loss=599947.45 active=668078 feature_norm=209.94\n",
      "Iter 132 time=16.26 loss=598516.47 active=668078 feature_norm=211.16\n",
      "Iter 133 time=8.86  loss=595746.11 active=668078 feature_norm=213.19\n",
      "Iter 134 time=7.25  loss=594036.23 active=668078 feature_norm=213.50\n",
      "Iter 135 time=7.85  loss=592575.37 active=668078 feature_norm=213.05\n",
      "Iter 136 time=8.86  loss=590668.01 active=668078 feature_norm=212.53\n",
      "Iter 137 time=9.47  loss=587387.29 active=668078 feature_norm=212.84\n",
      "Iter 138 time=9.69  loss=583819.36 active=668078 feature_norm=213.65\n",
      "Iter 139 time=10.18 loss=582375.43 active=668078 feature_norm=213.66\n",
      "Iter 140 time=7.72  loss=580640.65 active=668078 feature_norm=213.77\n",
      "Iter 141 time=8.37  loss=578462.97 active=668078 feature_norm=214.24\n",
      "Iter 142 time=15.27 loss=576721.81 active=668078 feature_norm=214.73\n",
      "Iter 143 time=8.09  loss=574101.87 active=668078 feature_norm=215.36\n",
      "Iter 144 time=7.36  loss=571530.58 active=668078 feature_norm=216.37\n",
      "Iter 145 time=7.44  loss=570579.83 active=668078 feature_norm=216.72\n",
      "Iter 146 time=7.35  loss=567569.40 active=668078 feature_norm=218.16\n",
      "Iter 147 time=13.84 loss=566002.64 active=668078 feature_norm=218.94\n",
      "Iter 148 time=8.00  loss=563533.96 active=668078 feature_norm=219.73\n",
      "Iter 149 time=7.32  loss=560881.45 active=668078 feature_norm=220.78\n",
      "Iter 150 time=7.17  loss=559821.27 active=668078 feature_norm=221.15\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 1398.759\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 668078 (668078)\n",
      "Number of active attributes: 437537 (4138657)\n",
      "Number of active labels: 4 (4)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 1.837\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', keep_tempfiles=None, max_iterations=150, min_freq=10,\n",
       "    verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers, metrics\n",
    "\n",
    "def extract_sent_features(x):\n",
    "    sent_features = []\n",
    "    for i in range(len(x)):\n",
    "        sent_features.append(extract_char_features(x, i))\n",
    "    return sent_features\n",
    "    \n",
    "def extract_char_features(sent, position):\n",
    "    char_features = {}\n",
    "    for i in range(-3, 4):\n",
    "        if len(sent) > position + i >= 0:\n",
    "            char_features['char_at_%d' % i] = sent[position + i]\n",
    "\n",
    "    for i in range(-2, 4):\n",
    "        if len(sent)-1 > position + i >= 0:\n",
    "            char_features['char_at_%d_%d' % (i, i+1)] = sent[position + i] + sent[position + i+1]\n",
    "    return char_features\n",
    "\n",
    "crf_tagger = sklearn_crfsuite.CRF(algorithm='lbfgs', min_freq=10, max_iterations=150, verbose=True)\n",
    "\n",
    "feature_X = []\n",
    "for x in train_X:\n",
    "    feature_X.append(extract_sent_features(x))\n",
    "crf_tagger.fit(feature_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(actual_toks, pred_toks):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    p = 0\n",
    "    q = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    while i < len(actual_toks) and j < len(pred_toks):\n",
    "        if p == q:\n",
    "            if actual_toks[i] == pred_toks[j]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            p += len(actual_toks[i])\n",
    "            q += len(pred_toks[j])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif p < q:\n",
    "            p += len(actual_toks[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "            q += len(pred_toks[j])\n",
    "            j += 1\n",
    "    return tp, fp, len(actual_toks)\n",
    "    \n",
    "def score(actual_sents, pred_sents):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total = 0\n",
    "    for actual_toks, pred_toks in zip(actual_sents, pred_sents):\n",
    "        tp_, fp_, total_ = compare(actual_toks, pred_toks)\n",
    "        tp += tp_\n",
    "        fp += fp_\n",
    "        total += total_\n",
    "    recall = float(tp) / total\n",
    "    precision = float(tp) / (tp + fp)\n",
    "    f1 = 2.0 * recall * precision / (recall + precision)\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['法國', '總統', '馬克宏', '已', '到', '現場', '勘災', '，', '初步', '傳出', '火警', '可能', '與', '目前', '聖母院', '的', '維修', '工程', '有關', '。']\n"
     ]
    }
   ],
   "source": [
    "def segment(sent):\n",
    "    tags = crf_tagger.predict_single(extract_sent_features(list(sent)))\n",
    "    tokens = []\n",
    "    tok = \"\"\n",
    "    for ch, tag in zip(list(sent), tags):\n",
    "        if tag in ['S', 'L'] and tok != \"\":\n",
    "            tokens.append(tok)\n",
    "            tok = \"\"\n",
    "        tok += ch\n",
    "    if tok:\n",
    "        tokens.append(tok)\n",
    "    return tokens\n",
    "            \n",
    "print(segment(\"法國總統馬克宏已到現場勘災，初步傳出火警可能與目前聖母院的維修工程有關。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "['許多', '社區', '長青', '學苑', '多', '開', '設有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "(0.9266164285248255, 0.9166438079870916, 0.9216031407412214)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "for sent in raw_test:\n",
    "    pred.append(segment(\"\".join(sent)))\n",
    "    actual.append(sent)\n",
    "print(actual[0])\n",
    "print(pred[0])\n",
    "\n",
    "print(score(actual, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
